{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkHsxDYijRL5Q6VIc9BUKT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish26meshram/Assigment-5/blob/main/scrap_data_code5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRkFPBqUonwS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def extract_person_name(html_content):\n",
        "    # Patterns to identify person name\n",
        "    name_patterns = [\n",
        "        re.compile(r'<h1[^>]*>\\s*([^<]+)\\s*</h1>', re.IGNORECASE),\n",
        "        re.compile(r'<span class=\"me-2[^>]*>\\s*([^<]+)\\s*</span>', re.IGNORECASE),\n",
        "        re.compile(r'<font style=\"vertical-align: inherit;\">\\s*([^<]+)\\s*</font>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in name_patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return ''\n",
        "\n",
        "def extract_person_title(html_content):\n",
        "    # Patterns to identify person title\n",
        "    title_patterns = [\n",
        "        re.compile(r'<h1[^>]*>\\s*([^<]+)\\s*(?:<span class=\"[^\"]*\">([^<]+)</span>)?\\s*</h1>', re.IGNORECASE),\n",
        "        re.compile(r'<strong>\\s*([^<]+)\\s*</strong>', re.IGNORECASE),\n",
        "        re.compile(r'<p class=\"lawyer-titles\">\\s*([^<]+)\\s*</p>', re.IGNORECASE),\n",
        "        re.compile(r'<span class=\"card--role\">\\s*([^<]+)\\s*</span>', re.IGNORECASE),\n",
        "        re.compile(r'<p class=\"grade\">\\s*([^<]+)\\s*</p>', re.IGNORECASE),\n",
        "        re.compile(r'<h3 class=\"\">\\s*([^<]+)\\s*</h3>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in title_patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return ''\n",
        "\n",
        "def extract_person_email(html_content):\n",
        "    # Patterns to identify person email\n",
        "    email_patterns = [\n",
        "        re.compile(r'<a[^>]*href=\"mailto:([^\"]+)\"[^>]*>', re.IGNORECASE),\n",
        "        re.compile(r'<p>\\s*([^<]+)\\s*</p>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in email_patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return ''\n",
        "\n",
        "def extract_person_phone(html_content):\n",
        "    # Patterns to identify person phone number\n",
        "    phone_patterns = [\n",
        "        re.compile(r'<a[^>]*href=\"tel:([^\"]+)\"[^>]*>', re.IGNORECASE),\n",
        "        re.compile(r'<p>\\s*([^<]+)\\s*</p>', re.IGNORECASE),\n",
        "        re.compile(r'<div[^>]*>\\s*([^<]+)\\s*</div>', re.IGNORECASE),\n",
        "        re.compile(r'<font style=\"vertical-align: inherit;\">\\s*Tel:\\s*([^<]+)\\s*</font>', re.IGNORECASE),\n",
        "        re.compile(r'<span class=\"desktop\">\\s*([^<]+)\\s*</span>', re.IGNORECASE),\n",
        "        re.compile(r'\\bM\\b.*\\b\\+\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b', re.IGNORECASE),\n",
        "        re.compile(r'\\bPhone\\b.*\\b\\+\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b.*\\b\\d{2,}\\b', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in phone_patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return ''\n",
        "\n",
        "def extract_person_city(html_content):\n",
        "    # Patterns to identify person city\n",
        "    city_patterns = [\n",
        "        re.compile(r'<span class=\"icon location\">([^<]+)</span>', re.IGNORECASE),\n",
        "        re.compile(r'<div class=\"info_person_content\">.*?<b>([^<]+)</b>', re.IGNORECASE),\n",
        "        re.compile(r'<a[^>]*class=\"text-decoration-underline-hover[^\"]*\"[^>]*>\\s*([^<]+)\\s*</a>', re.IGNORECASE),\n",
        "        re.compile(r'<p>\\s*([^<]+)\\s*</p>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in city_patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return ''\n",
        "\n",
        "def extract_practice_area(html_content):\n",
        "    # Patterns to identify practice areas\n",
        "    practice_area_patterns = [\n",
        "        re.compile(r'\\b(?:Areas of focus|Key Activities|Technical Expertise|Technical focus|Experience|Overview|Detailed profile|Legal expertises|Focus of legal work|Training|Technical areas)\\b[^>]*>\\s*([^<]+)\\s*<', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    practice_areas = []\n",
        "    for pattern in practice_area_patterns:\n",
        "        matches = pattern.finditer(html_content)\n",
        "        for match in matches:\n",
        "            practice_areas.append(match.group(1).strip())\n",
        "\n",
        "    return practice_areas\n",
        "\n",
        "def extract_legal_activities(html_content):\n",
        "    # Keywords for legal activities\n",
        "    prosecution_keywords = ['Prosecution', 'Application Perpetration', 'Drafting', 'Filing', 'patent Filing',\n",
        "                            'Negotiating', 'Patentability Prosecutions and registration', 'Protection',\n",
        "                            'Patent Searches', 'Novelty (Patentability)', 'Freedom To Operate (FTO)',\n",
        "                            'Non-Infringement', 'Validity', 'patent analyses', 'Patent Application Proceeding',\n",
        "                            'Patent counselling', 'patent analysing']\n",
        "\n",
        "    litigation_keywords = ['Litigation', 'Infringement', 'infringement cases', 'enforcement']\n",
        "\n",
        "    licensing_keywords = ['licensing', 'licenses', 'License']\n",
        "\n",
        "    prosecution_found = any(keyword.lower() in html_content.lower() for keyword in prosecution_keywords)\n",
        "    litigation_found = any(keyword.lower() in html_content.lower() for keyword in litigation_keywords)\n",
        "    licensing_found = any(keyword.lower() in html_content.lower() for keyword in licensing_keywords)\n",
        "\n",
        "    return {\n",
        "        'Prosecution': 'Prosecution' if prosecution_found else 'Not Found',\n",
        "        'Litigation': 'Litigation' if litigation_found else 'Not Found',\n",
        "        'Licensing': 'Licensing' if licensing_found else 'Not Found',\n",
        "    }\n",
        "\n",
        "def scrape_person_data(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "    except requests.exceptions.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "        return {}\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "        return {}\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "        return {}\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print(f\"Error: {err}\")\n",
        "        return {}\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extracting person name\n",
        "    person_name_element = soup.find_all(['h1', 'span', 'font'], text=True)\n",
        "    person_name_content = ' '.join([str(element) for element in person_name_element])\n",
        "    person_name = extract_person_name(person_name_content)\n",
        "\n",
        "    # Extracting person title\n",
        "    person_title_element = soup.find_all(['h1', 'strong', 'p', 'span', 'h3'], text=True)\n",
        "    person_title_content = ' '.join([str(element) for element in person_title_element])\n",
        "    person_title = extract_person_title(person_title_content)\n",
        "\n",
        "    # Extracting person email\n",
        "    person_email_element = soup.find_all(['a', 'p'], text=True)\n",
        "    person_email_content = ' '.join([str(element) for element in person_email_element])\n",
        "    person_email = extract_person_email(person_email_content)\n",
        "\n",
        "    # Extracting person phone\n",
        "    person_phone_element = soup.find_all(['a', 'p', 'div', 'font', 'span'], text=True)\n",
        "    person_phone_content = ' '.join([str(element) for element in person_phone_element])\n",
        "    person_phone = extract_person_phone(person_phone_content)\n",
        "\n",
        "    # Extracting person city\n",
        "    person_city_element = soup.find_all(['span', 'div', 'a', 'p'], text=True)\n",
        "    person_city_content = ' '.join([str(element) for element in person_city_element])\n",
        "    person_city = extract_person_city(person_city_content)\n",
        "\n",
        "    # Extracting practice areas\n",
        "    practice_area_element = soup.find_all(['h2', 'h3', 'h1', 'p', 'font'], text=True)\n",
        "    practice_area_content = ' '.join([str(element) for element in practice_area_element])\n",
        "    practice_areas = extract_practice_area(practice_area_content)\n",
        "\n",
        "    # Extracting legal activities\n",
        "    legal_activities_content = ' '.join([str(element) for element in soup.find_all(['h2', 'h3', 'h1', 'p', 'font'], text=True)])\n",
        "    legal_activities = extract_legal_activities(legal_activities_content)\n",
        "\n",
        "    # Check for keywords in the extracted content\n",
        "    keywords = ['IP', 'Patent', 'Trademark']\n",
        "    ip_found = any(keyword.lower() in ' '.join(practice_areas).lower() for keyword in keywords[:1])\n",
        "    patent_found = any(keyword.lower() in ' '.join(practice_areas).lower() for keyword in keywords[1:2])\n",
        "    trademark_found = any(keyword.lower() in ' '.join(practice_areas).lower() for keyword in keywords[2:])\n",
        "\n",
        "    return {\n",
        "        'Url_link': url,  # Link column\n",
        "        'Persone_name': person_name,\n",
        "        'persone_Title': person_title,\n",
        "        'persone_Email_id': person_email,\n",
        "        'IP': 'IP' if ip_found else 'Not Found',\n",
        "        'Patent': 'Patent' if patent_found else 'Not Found',\n",
        "        'Treadmakr': 'Trademark' if trademark_found else 'Not Found',\n",
        "        'Prosecution': legal_activities['Prosecution'],\n",
        "        'Litigation': legal_activities['Litigation'],\n",
        "        'Licensing': legal_activities['Licensing'],\n",
        "        'persone phone number': person_phone,\n",
        "        'Persone_city': person_city\n",
        "    }\n",
        "\n",
        "def scrape_multiple_links(urls):\n",
        "    data_list = []\n",
        "    for url in urls:\n",
        "        person_data = scrape_person_data(url)\n",
        "        if person_data:\n",
        "            data_list.append(person_data)\n",
        "\n",
        "    return data_list\n",
        "\n",
        "def save_to_csv(data_list, csv_file_path):\n",
        "    if not data_list:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(['Url_link', 'Persone_name', 'persone_Title', 'persone_Email_id', 'IP', 'Patent', 'Treadmakr', 'Prosecution', 'Litigation', 'Licensing', 'persone phone number', 'Persone_city'])\n",
        "        for data in data_list:\n",
        "            csv_writer.writerow([data[col] for col in ['Url_link', 'Persone_name', 'persone_Title', 'persone_Email_id', 'IP', 'Patent', 'Treadmakr', 'Prosecution', 'Litigation', 'Licensing', 'persone phone number', 'Persone_city']])\n",
        "\n",
        "def main():\n",
        "    # Example URLs\n",
        "    urls = [\n",
        "        'https://example.com/person1',\n",
        "        'https://example.com/person2',\n",
        "        'https://example.com/person3',\n",
        "        # Add more URLs as needed\n",
        "    ]\n",
        "\n",
        "    data_list = scrape_multiple_links(urls)\n",
        "    save_to_csv(data_list, 'output_data.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def extract_person_name(html_content):\n",
        "    # Add patterns to identify person name\n",
        "    patterns = [\n",
        "        re.compile(r'<h1[^>]*>(.*?)</h1>', re.IGNORECASE),\n",
        "        re.compile(r'<span[^>]*class=[\"\\']?me-2[^>]*>(.*?)</span>', re.IGNORECASE),\n",
        "        re.compile(r'<font[^>]*style=[\"\\']?vertical-align: inherit;[^>]*>(.*?)</font>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return 'Not Found'\n",
        "\n",
        "def extract_person_title(html_content):\n",
        "    # Add patterns to identify person title\n",
        "    patterns = [\n",
        "        re.compile(r'<h1[^>]*>(.*?)</h1>', re.IGNORECASE),\n",
        "        re.compile(r'<strong[^>]*>(.*?)</strong>', re.IGNORECASE),\n",
        "        re.compile(r'<p[^>]*class=[\"\\']?card--role[^>]*>(.*?)</p>', re.IGNORECASE),\n",
        "        re.compile(r'<span[^>]*class=[\"\\']?styles__type__position[^>]*>(.*?)</span>', re.IGNORECASE),\n",
        "        re.compile(r'<h3[^>]*class=[\"\\']?page-attorney-headline[^>]*>(.*?)</h3>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return 'Not Found'\n",
        "\n",
        "def extract_person_email(html_content):\n",
        "    # Add patterns to identify person email\n",
        "    patterns = [\n",
        "        re.compile(r'<a[^>]*href=[\"\\']?mailto:(.*?)[\"\\']?[^>]*>', re.IGNORECASE),\n",
        "        re.compile(r'<p[^>]*>[^<]*[Ee]-?mail[^<]*:([^<]*)</p>', re.IGNORECASE),\n",
        "        re.compile(r'<span[^>]*class=[\"\\']?desktop email--obfuscate[^>]*>(.*?)</span>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return 'Not Found'\n",
        "\n",
        "def extract_person_phone(html_content):\n",
        "    # Add patterns to identify person phone number\n",
        "    patterns = [\n",
        "        re.compile(r'<a[^>]*href=[\"\\']?tel:(.*?)[\"\\']?[^>]*>', re.IGNORECASE),\n",
        "        re.compile(r'<p[^>]*>Tel[^<]*:([^<]*)</p>', re.IGNORECASE),\n",
        "        re.compile(r'<div[^>]*class=[\"\\']?left_assign[^>]*>(.*?)</div>', re.IGNORECASE),\n",
        "        re.compile(r'\\b(?:\\+?\\d{1,4}[-.●])?(?:\\(\\d{1,4}\\)[-.\\●])?\\d{1,12}[-.●]?\\d{1,12}[-.●]?\\d{1,9}\\b', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return 'Not Found'\n",
        "\n",
        "def extract_person_city(html_content):\n",
        "    # Add patterns to identify person city\n",
        "    patterns = [\n",
        "        re.compile(r'<span[^>]*class=[\"\\']?icon location[^>]*>(.*?)</span>', re.IGNORECASE),\n",
        "        re.compile(r'<a[^>]*class=[\"\\']?styles__type__officeName[^>]*>(.*?)</a>', re.IGNORECASE),\n",
        "        re.compile(r'<p[^>]*>([^<]*?office[^<]*?)</p>', re.IGNORECASE),\n",
        "        re.compile(r'\\b(?:London|New York|Germany|Australia)\\b', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(html_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    return 'Not Found'\n",
        "\n",
        "def extract_practice_area(html_content):\n",
        "    # Add patterns to identify practice areas\n",
        "    patterns = [\n",
        "        re.compile(r'<h2[^>]*>(.*?)</h2>', re.IGNORECASE),\n",
        "        re.compile(r'<h3[^>]*>(.*?)</h3>', re.IGNORECASE),\n",
        "        re.compile(r'<h1[^>]*>(.*?)</h1>', re.IGNORECASE),\n",
        "        re.compile(r'<p[^>]*class=[\"\\']?lawyer-titles[^>]*>(.*?)</p>', re.IGNORECASE),\n",
        "        re.compile(r'<font[^>]*style=[\"\\']?vertical-align: inherit;[^>]*>(.*?)</font>', re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    practice_areas = []\n",
        "    for pattern in patterns:\n",
        "        matches = pattern.finditer(html_content)\n",
        "        for match in matches:\n",
        "            practice_areas.append(match.group(1).strip())\n",
        "\n",
        "    return practice_areas\n",
        "\n",
        "def extract_legal_activities(html_content):\n",
        "    # Keywords for legal activities\n",
        "    prosecution_keywords = ['Prosecution', 'Application Perpetration', 'Drafting', 'Filing', 'patent Filing',\n",
        "                            'Negotiating', 'Patentability Prosecutions and registration', 'Protection',\n",
        "                            'Patent Searches', 'Novelty (Patentability)', 'Freedom To Operate (FTO)',\n",
        "                            'Non-Infringement', 'Validity', 'patent analyses', 'Patent Application Proceeding',\n",
        "                            'Patent counselling', 'patent analysing']\n",
        "\n",
        "    litigation_keywords = ['Litigation', 'Infringement', 'infringement cases', 'enforcement']\n",
        "\n",
        "    licensing_keywords = ['licensing', 'licenses', 'License']\n",
        "\n",
        "    prosecution_found = any(keyword.lower() in html_content.lower() for keyword in prosecution_keywords)\n",
        "    litigation_found = any(keyword.lower() in html_content.lower() for keyword in litigation_keywords)\n",
        "    licensing_found = any(keyword.lower() in html_content.lower() for keyword in licensing_keywords)\n",
        "\n",
        "    return {\n",
        "        'Prosecution': 'Prosecution' if prosecution_found else 'Not Found',\n",
        "        'Litigation': 'Litigation' if litigation_found else 'Not Found',\n",
        "        'Licensing': 'Licensing' if licensing_found else 'Not Found',\n",
        "    }\n",
        "\n",
        "def scrape_person_data(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "    except requests.exceptions.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "        return {}\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "        return {}\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "        return {}\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print(f\"Something went wrong: {err}\")\n",
        "        return {}\n",
        "\n",
        "    html_content = response.content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    person_name = extract_person_name(str(soup))\n",
        "    person_title = extract_person_title(str(soup))\n",
        "    person_email = extract_person_email(str(soup))\n",
        "    person_phone = extract_person_phone(str(soup))\n",
        "    person_city = extract_person_city(str(soup))\n",
        "    practice_areas = extract_practice_area(str(soup))\n",
        "    legal_activities = extract_legal_activities(str(soup))\n",
        "\n",
        "    data = {\n",
        "        'Url_link': url,\n",
        "        'Persone_name': person_name,\n",
        "        'persone_Title': person_title,\n",
        "        'persone_Email_id': person_email,\n",
        "        'IP': ', '.join(practice_areas),\n",
        "        'Patent': ', '.join(practice_areas),\n",
        "        'Treadmakr': ', '.join(practice_areas),\n",
        "        **legal_activities,\n",
        "        'persone phone number': person_phone,\n",
        "        'Persone_city': person_city\n",
        "    }\n",
        "\n",
        "    return data\n",
        "\n",
        "def scrape_multiple_links_from_csv(csv_file_path):\n",
        "    data_list = []\n",
        "\n",
        "    with open(csv_file_path, 'r') as csv_file:\n",
        "        reader = csv.DictReader(csv_file)\n",
        "        for row in reader:\n",
        "            url = row['Link']\n",
        "            scraped_data = scrape_person_data(url)\n",
        "            data_list.append(scraped_data)\n",
        "\n",
        "    return data_list\n",
        "\n",
        "def save_to_csv(data_list, output_file):\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        csv_writer = csv.DictWriter(csv_file, fieldnames=['Url_link', 'Persone_name', 'persone_Title', 'persone_Email_id',\n",
        "                                                           'IP', 'Patent', 'Treadmakr', 'Prosecution', 'Litigation',\n",
        "                                                           'Licensing', 'persone phone number', 'Persone_city'])\n",
        "        csv_writer.writeheader()\n",
        "        for data in data_list:\n",
        "            csv_writer.writerow(data)\n",
        "\n",
        "def main():\n",
        "    input_csv_file = 'input_links.csv'\n",
        "    output_csv_file = 'output_data.csv'\n",
        "\n",
        "    data_list = scrape_multiple_links_from_csv(input_csv_file)\n",
        "    save_to_csv(data_list, output_csv_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "c7mBq-IGo4sh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}